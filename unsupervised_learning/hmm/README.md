# Hidden Markov Models

Hidden Markov Models (HMMs) are a class of probabilistic graphical model that are used to model sequences of data. They are used in a wide range of applications, including speech recognition, bioinformatics, and finance.

In an HMM, we have a sequence of observations, which are generated by a sequence of hidden states. The hidden states are not directly observable, but we can infer them from the observations. The hidden states form a Markov chain, where the probability of transitioning from one state to another depends only on the current state. The observations are generated by the hidden states according to a probability distribution.

HMMs are typically trained using the Baum-Welch algorithm, which is a variant of the Expectation-Maximization (EM) algorithm. The Baum-Welch algorithm iteratively estimates the parameters of the HMM (transition probabilities and emission probabilities) by maximizing the likelihood of the observed data.

Once an HMM has been trained, we can use it to make predictions about the hidden states given a sequence of observations. This is done using the Viterbi algorithm, which finds the most likely sequence of hidden states that generated the observations.

HMMs are a powerful tool for modeling sequential data, and they have been successfully applied to a wide range of problems. They are particularly useful when the data has a sequential structure and the underlying process is not directly observable.

In this section, we will explore the theory behind HMMs, as well as practical examples of how they can be used in various applications. We will also discuss some of the limitations of HMMs and alternative approaches that can be used to model sequential data.

# TASKS

| #  | Task                                                 | Description                                                                                        |
|----|------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| 0  | [Markov Chain](./0-markov_chain.py) | Determines  the probability of a markov chain being in a particular state after a specified number of iterations |
| 1  | [Regular Chains](./1-regular.py) | Determines the steady state probabilities of a regular markov chain |
| 2  | [Absorbing Chains](./2-absorbing.py) | Determines the steady state probabilities of an absorbing markov chain |
| 3  | [The Forward Algorithm](./3-forward.py) | Calculates the likelihood of a sequence of observations given a hidden Markov model |
| 4  | [The Viretbi Algorithm](./4-viterbi.py) | Determines the most likely sequence of hidden states for a sequence of observations |
| 5  | [The Backward Algorithm](./5-backward.py) | Calculates the likelihood of a sequence of observations given a hidden Markov model |
| 6  | [The Baum-Welch Algorithm](./6-baum_welch.py) | Trains a hidden Markov model using the Baum-Welch algorithm |

