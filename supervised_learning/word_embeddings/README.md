# Natural Language Processing - Word Embeddings

## Introduction

Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors are learned in such a way that words that have similar meanings will have similar representations in the vector space. Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors are learned in such a way that words that have similar meanings will have similar representations in the vector space.

## TASKS

| Task                                       | Description                                                 |
|--------------------------------------------|-------------------------------------------------------------|
| [Bag of Words](./0-bag_of_words.py)        | Creates a bag of words embedding matrix                     |
 | [TF-IDF](./1-tf_idf.py)                    | Creates a TF-IDF embedding matrix                           |
| [Train Word2Vec](./2-word2vec.py)          | Creates and trains a Word2Vec model                         |
| [Extract Word2Vec](./3-gensim_to_keras.py) | Converts a gensim word2vec model to a keras Embedding layer |
| [FastText](./4-fasttext.py)                | Creates and trains a FastText model                         |
| [ELMO](./5-elmo)                           | Answer some questions about ELMO                            |

